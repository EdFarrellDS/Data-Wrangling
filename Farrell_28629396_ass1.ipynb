{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name: Ed Farrell\n",
    "#### Student ID: 28629396\n",
    "\n",
    "Date: 30/08/2017\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 2.7.13 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* re (for regular expression, included in Anaconda Python 2.7)\n",
    "* os (for directory management and creation)\n",
    "* ElementTree (for parsing XML files)\n",
    "* nltk (for text processing of abstracts; *probability*, *corpus* and *util* modules used)\n",
    "* collections (for Counter function)\n",
    "* copy (for creating deep copies of variables)\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This assignment aims to extract information from a corpus of 2500 patents stored as XML files. Off the bat, the format of the original 'patents.xml' file raised errors as it was a collection of XML files saved to a single file, rather than an individual XML file. Following a visual inspection of the original file via Notepad++, the patents were split into individual XML files prior to parsing. Given the complex structure of XML files, ElemetTree's ability to parse XML data was key in extracting the relevant information as required by the assignment description.\n",
    "\n",
    "ElementTree was chosen for its simplicity, with RegEx being used for its ability to quickly and effectively manipulate strings.\n",
    "The use of dictionaries with list values was chosen throughout the assignment due to the ability to store a single unique key (in this case, a patent ID) with an effectively unlimited, and more importantly mutable, storage volume as the value. By using a single list, the program is able to store a single string, the tokenised version of that string, a list of lists, etc. etc.\n",
    "Due to this use of dictionaries, the use of dedicated dataframe packages such as Numpy or Pandas was disregarded as they were identified as providing the same service without the benefit of being a built-in python datatype.\n",
    "\n",
    "__ PLEASE NOTE: THIS PROGRAM WAS BUILT ON A WINDOWS OS, AND ASSUMES THAT IT WILL BE RUN / MARKED ON ONE. THIS PROGRAM CREATES SUBDIRECTORIES; IF FOR WHATEVER REASON YOU DO NOT HAVE PERMISSION / THE ABILITY TO PERFORM THIS ON YOUR DEVICE, PLEASE ATTEMPT ON ONE THAT DOES.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Key initial imports\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Parsing tools\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.probability import *\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords as Stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Miscellaneous libraries\n",
    "from collections import Counter\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Declare top-level variables & create subdirectories\n",
    "\n",
    "This includes variables that determine the patent file being read, which stopwords store is used, and which data extraction paths are to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assigns the patent file to the variable patent\n",
    "patent = 'patents.xml'\n",
    "\n",
    "# Path variables for data extraction\n",
    "patent_cites_path = 'us-bibliographic-data-grant/references-cited/citation/patcit/document-id'\n",
    "IPC_path = 'us-bibliographic-data-grant/classifications-ipcr/classification-ipcr'\n",
    "abstract_path = 'abstract'\n",
    "\n",
    "# Target variables for data extraction\n",
    "target_IPC = ['section', 'class', 'subclass', 'main-group', 'subgroup']\n",
    "target_cites = ['doc-number']\n",
    "target_patents = ['p']\n",
    "\n",
    "# Stopword declaration; this list is sourced from NLTK\n",
    "stopwords_NLTK = Stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patent sub-directory created.\n",
      "Text-files sub-directory created.\n"
     ]
    }
   ],
   "source": [
    "# Checks to see whether folders named 'Patents' and 'Text-files' exist, and if not then the necessary folders are created.\n",
    "# This will be used to store individual patent xml files once they have been split from the patents.xml file, and the required\n",
    "# .txt files storing extracted data records as outlined in the Assignment description.\n",
    "if os.path.exists(\"Patents\"):\n",
    "    print \"Patent sub-directory found.\"\n",
    "else:\n",
    "    os.makedirs(\"Patents\")\n",
    "    print \"Patent sub-directory created.\"\n",
    "if os.path.exists(\"Text-files\"):\n",
    "    print \"Text-files sub-directory found.\"\n",
    "else:\n",
    "    os.makedirs(\"Text-files\")\n",
    "    print \"Text-files sub-directory created.\"\n",
    "\n",
    "# Creates variables storing paths for the Patents and Text-files subdirectories\n",
    "xml_subdir = os.path.join(os.getcwd(), \"Patents\")\n",
    "txt_subdir = os.path.join(os.getcwd(), \"Text-files\")\n",
    "\n",
    "# Variable containing the names of each .txt file that will store extracted data\n",
    "text_file_names = ['classification.txt', 'citations.txt', 'cited.txt', 'count_vectors.txt', 'vocab.txt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define program-wide functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function used to extract information from each patent in a list of patent files. This function assumes the following;\n",
    "# 1. Each patent is an individual .xml file, and which conforms to a pre-defined path structure (as per the path argument)\n",
    "# 2. Each extraction is for information on a single level; information across multiple levels will require separate runs of\n",
    "# dataTrawler.\n",
    "\n",
    "def dataTrawler(patents, path, targets):\n",
    "    # Variable declarations - patent_data is filled with data trawled from patents, and then is appended to complete_data when\n",
    "    # the trawling is finished for a particular patent.\n",
    "    complete_data = []\n",
    "    patent_data = []\n",
    "    \n",
    "    # Iterates over all patents in 'patents', setting a root and then descending on the path nominated in the path argument.\n",
    "    # Once at the final level, the function pulls all text data associated with the targets stated in the target argument,\n",
    "    # appending each occurence to the patent_data variable.\n",
    "    for patent in patents:\n",
    "        tree = etree.parse(patent)\n",
    "        root = tree.getroot()\n",
    "        for element in root.findall(path):\n",
    "            for data in element:\n",
    "                if data.tag in targets:\n",
    "                    if data.tag == 'p':\n",
    "                        if data.attrib['id'] == 'p-0001':\n",
    "                            patent_data.append(data.text)\n",
    "                    else:\n",
    "                        patent_data.append(data.text)\n",
    "        complete_data.append(patent_data)\n",
    "        patent_data = []\n",
    "    \n",
    "    # Creates a dictionary variable, where each patent name is a key and the value of each key is a list of the corresponding\n",
    "    # extracted IPC code details.\n",
    "    data_in_dict = dict(zip(patent_names, complete_data))\n",
    "    return data_in_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extracting data and writing to individual files\n",
    "\n",
    "As ElementTree is being used to parse the XML data, the concatenated series of XML files in __'patents.xml'__ are split into individual patent files that are then iterated through by ElementTree & the function __dataTrawler__ (defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pulls data from patents.xml into a list of lists, with each sublist containing an individual patent where each line\n",
    "# is an element of the sublist.\n",
    "\n",
    "# Variable declarations\n",
    "single_patent = []\n",
    "all_patents = []\n",
    " \n",
    "with open(patent) as patents:\n",
    "    for line in patents:\n",
    "        if not line.startswith(\"<?xml version=\"):\n",
    "            single_patent.append(line)\n",
    "        else:\n",
    "            # If this is the first occurence of '<?xml version=', then write this line to single_patent\n",
    "            if len(single_patent) == 0:\n",
    "                single_patent.append(line)\n",
    "            else:\n",
    "            # If this is not the first occurence of '<?xml version=', then append single_patent to all_patents, wipe single_patent,\n",
    "            # and then write the line to single_patent. In essence, transfer the previous patent to all_patents and then\n",
    "            # over-write the contents of the list.\n",
    "                all_patents += ([single_patent])\n",
    "                single_patent = []\n",
    "                single_patent.append(line)\n",
    "    # Writes the last patent in the xml file to all_patents, as this would not have been done otherwise.\n",
    "    all_patents.append(single_patent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a list containing the document names for each patent in patents.xml, as well as a list containing the patent names.\n",
    "patent_names = []\n",
    "for item in all_patents:\n",
    "    for i in item:\n",
    "        patent_filename = re.findall(r\"\\w{10}\\-\\w{8}\\.XML\", i)\n",
    "        if len(patent_filename) > 0:\n",
    "            patent_names.append(patent_filename)\n",
    "patent_names = [i[:10] for j in patent_names for i in j] # Flattens the list of lists, and cleans the patent name information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 patent files separated.\n"
     ]
    }
   ],
   "source": [
    "# Creates an individual xml file for each patent file stored in patents.xml\n",
    "# A list comprehension is used to generate the filenames (including directory path) for all patent names stored in the\n",
    "# patent_names variable.\n",
    "list_of_xmls = [xml_subdir+\"\\\\\"+ name+\".XML\" for name in patent_names]\n",
    "count = 0\n",
    "for document in list_of_xmls:\n",
    "    contents = all_patents[count]\n",
    "    newfile = open(document, 'w')\n",
    "    newfile.write(\"\".join(contents))\n",
    "    count += 1\n",
    "    newfile.close()\n",
    "\n",
    "print count, \"patent files separated.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parse the XML files and create text files containing extracted data\n",
    "\n",
    "Four text files are created below. These files are detailed as follows;\n",
    "1. __'classifications.txt'__ - contains each patent and its related heirarchical IPC code, in the order Section, Class, Subclass, Main Group and Subgroup\n",
    "2. __'citations.txt'__ - contains each patent, and each existent patent cited by that patent\n",
    "3. __'citations_count.txt'__ - contains each patent, and the number of patents cited by that patent. This is Mohsen Laali's interpretation of the 'cited.txt' requirements.\n",
    "4. __'cited.txt'__ - contains each patent cited by the patents in the original 'patents.xml' file, as well as a count of the number of times that patent is cited within the whole body of patents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'classifications.txt' successfully written to file with 2500 lines written.\n"
     ]
    }
   ],
   "source": [
    "# Variable used to store each patent's IPC code records as a dictionary, where each key is a patent and each value is a list\n",
    "# containing the IPC codes in the order section, class, subclass, main-group, and subgroup.\n",
    "IPC_data = dataTrawler(list_of_xmls, IPC_path, target_IPC)\n",
    "\n",
    "# Writes the contents of IPC_data to the text file classifications.txt\n",
    "count = 0\n",
    "with open(txt_subdir+\"\\\\classifications.txt\", 'w') as classification_file:\n",
    "    for i,j in IPC_data.iteritems():\n",
    "        count += 1\n",
    "        classification_file.write(i+':'+str(j).replace('\\'', '').strip('[]')+'\\n')\n",
    "\n",
    "print \"'classifications.txt' successfully written to file with\", count, \"lines written.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'citations.txt' successfully written to file with 2500 lines written.\n"
     ]
    }
   ],
   "source": [
    "# Variable used to store the patents referenced by each patent as a dictionary, where each key is a patent and each value is a\n",
    "# patent referenced by the key patent.\n",
    "patents_cited_data = dataTrawler(list_of_xmls, patent_cites_path, target_cites)\n",
    "\n",
    "# Writes the contents of patents_cited_data to the text file citations.txt\n",
    "count = 0\n",
    "with open(txt_subdir+\"\\\\citations.txt\", 'w') as patents_cited_file:\n",
    "    for i,j in patents_cited_data.iteritems():\n",
    "        count += 1\n",
    "        patents_cited_file.write(i+':'+str(j).replace('\\'', '').strip('[]')+'\\n')\n",
    "\n",
    "print \"'citations.txt' successfully written to file with\", count, \"lines written.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'citation_count.txt' successfully written to file with 2500 lines written.\n"
     ]
    }
   ],
   "source": [
    "# Counts the number of citations that each patent cites, then zips the count to the patent number in a patent:count dictionary.\n",
    "citations_count = []\n",
    "for i, j in patents_cited_data.iteritems():\n",
    "    citations_count.append([len(j)])\n",
    "citations_count_dict = dict(zip(patents_cited_data, citations_count))\n",
    "\n",
    "# Writes the contents of citations_count_dict to the text file citation_count.txt\n",
    "count = 0\n",
    "with open(txt_subdir+\"\\\\citation_count.txt\", 'w') as patent_citations_file:\n",
    "    for i,j in citations_count_dict.iteritems():\n",
    "        count += 1\n",
    "        patent_citations_file.write(i+':'+str(j).replace('\\'', '').strip('[]')+'\\n')\n",
    "print \"'citation_count.txt' successfully written to file with\", count, \"lines written.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cited.txt' successfully written to file with 40728 lines written.\n"
     ]
    }
   ],
   "source": [
    "# Collects a list of each patent cited by <patent>.xml files, as seen in the values from the key:value relationships\n",
    "# in 'citations.txt'.\n",
    "patent_citations = []\n",
    "for key, value in patents_cited_data.iteritems():\n",
    "    patent_citations.append(value)\n",
    "patent_citations = [i for j in patent_citations for i in j]\n",
    "\n",
    "# For each patent in a set of the patents cited in each <patent>.xml file, append a dictionary containing the patent cited and the\n",
    "# number of times it has been cited by <patent>.xml files. A set is used to ensure that each patent cited only appears as a key\n",
    "# once, rather than every time that it appears as a citation in a <patent>.xml file.\n",
    "all_patent_count = []\n",
    "for patent in set(patent_citations):\n",
    "    patent_record = {patent: patent_citations.count(patent)}\n",
    "    all_patent_count.append(patent_record)\n",
    "\n",
    "# Writes the details from all_patent_count to a text file named 'cited.txt', with the count printed to the terminal providing\n",
    "# the number of patents referenced by the 2500 <patent>.xml files\n",
    "count = 0\n",
    "with open(txt_subdir+\"\\\\cited.txt\", 'w') as citation_count_file:\n",
    "    for count_vector in all_patent_count:\n",
    "        for i,j in count_vector.iteritems():\n",
    "            count += 1\n",
    "            citation_count_file.write(i+':'+str(j).replace('\\'', '').strip('[]')+'\\n')\n",
    "\n",
    "print \"'cited.txt' successfully written to file with\", count, \"lines written.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Abstract extraction\n",
    "\n",
    "This section extracts the abstract data from each individual patent, and provides the following variables;\n",
    "1. A list of lists __(all_token_list)__, where each sub-list contains a tokenised abstract for each patent\n",
    "2. A list of lists __(all_uniq_list)__, where each sub-list contains a tokenised abstract for each patent where each token is unique\n",
    "3. A list __(unique_tokens)__ containing tokens that only appear in a single patent (regardless of the number of times it appears in that patent)\n",
    "4. A list __(flat_token_list)__ that is a flattened variant of the list of lists all_token_list\n",
    "5. A dictionary __(pat_tokens_cleaned)__, where each key-value pair is a patent matched to a list containing the patent's abstract once it has been tokenised, and has been stripped of stopwords and unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5427 unique tokens in the corpus.\n"
     ]
    }
   ],
   "source": [
    "# Tokenise abstract for each patent file; each patent's abstract is contained as a key:value pair in abstract_data, with each\n",
    "# patent's unique tokens being stored as elements of all_uniq_list. This list will be used to establish whether a word appears\n",
    "# in only one patent.\n",
    "abstract_data = dataTrawler(list_of_xmls, abstract_path, target_patents)\n",
    "all_uniq_list = []\n",
    "all_token_list = []\n",
    "\n",
    "for i,j in abstract_data.iteritems():\n",
    "    # Declares variables for holding all tokens in an abstract, and a list of unique tokens for an abstract.\n",
    "    indiv_uniq_token = []\n",
    "    indiv_token_list = j\n",
    "    patent_num = copy.deepcopy(i)\n",
    "\n",
    "    # Iterates through the abstracts, extracting tokens for each word and for the punctuation marks '.' and ','. These are\n",
    "    # extracted so that bigram calculations can take into account common delimiters.\n",
    "    for string in indiv_token_list:\n",
    "        if isinstance(string, (str, unicode)):\n",
    "            tokens = re.findall(r\"[a-zA-Z-]+\", string.lower().encode('utf-8'))\n",
    "        else:\n",
    "            print \"A non-text token has been caught. Details are as follows; \\n type:\", type(string), patent_num, string\n",
    "    \n",
    "    # Creates a list with only one of each token occuring in the patent, and then appends this list \n",
    "    indiv_uniq_token = set(tokens)\n",
    "    \n",
    "    # Appends each unique token to the list all_uniq_list. A count run on this variable will return how often each term in the\n",
    "    # list occurs, i.e. how many patents it occurs in. Given this information, if the count returns as 1 then the word only\n",
    "    # appears in one patent, and can therefore be removed.\n",
    "    for uniq_token in indiv_uniq_token:\n",
    "        all_uniq_list.append(uniq_token)\n",
    "    \n",
    "    # Appends the list of token for each patent into the list of lists all_token_list.\n",
    "    all_token_list.append(tokens)\n",
    "\n",
    "# Flattens all_token_list, and also handles unicode encoding if it occurs.\n",
    "flat_token_list = [i for j in all_token_list for i in j]\n",
    "\n",
    "# Creates a list of tokens that only appear in a single patent; these tokens can therefore be removed from the count vector\n",
    "# vocabulary list. Note that counter returns a dictionary for each element in the list all_uniq_list, where the key is the\n",
    "# element and value is the number of occurences of the element in all_uniq_list.\n",
    "unique_tokens = []\n",
    "for key, value in Counter(all_uniq_list).iteritems():\n",
    "    if value == 1:\n",
    "        unique_tokens.append(key)\n",
    "\n",
    "print \"There are\",len(unique_tokens),\"unique tokens in the corpus.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a dictionary where each key is a patent and the value is a list of the patent abstract once it has been tokenised. The \n",
    "# variable pat_tokens_raw contains just the tokenised abstracts as elements of the list, while pat_tokens_cleaned has had any\n",
    "# stopwords and tokens present in only one patent removed.\n",
    "\n",
    "pat_tokens_raw = dataTrawler(list_of_xmls, abstract_path, target_patents)\n",
    "pat_tokens_cleaned = copy.deepcopy(pat_tokens_raw)\n",
    "\n",
    "for patent, abstract in pat_tokens_raw.iteritems():\n",
    "    for i in abstract:\n",
    "        string = i.encode('utf-8') # Encodes text to type str to deal with Python 2.7's inherent issues with Unicode characters.\n",
    "        tokens = [token for token in re.findall(r\"[a-zA-Z-]+\", string.lower())] #|[.,]\n",
    "    pat_tokens_raw[patent] = tokens\n",
    "\n",
    "# Cleans pat_tokens_raw of stopwords, and words that appear in only one patent.\n",
    "pat_tokens_cleaned[patent] = [token for token in pat_tokens_raw if token not in unique_tokens if token not in stopwords_NLTK]\n",
    "\n",
    "# Cleans flat_token_list of stopwords, and words that appear in only one patent. While pat_tokens_cleaned contains abstract\n",
    "# tokens for each patent, cleaned_flat_token_list contains the contents of all patent abstracts in the corpus.\n",
    "cleaned_flat_token_list = [token for token in flat_token_list if token not in stopwords_NLTK]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Abstract processing\n",
    "This section processes the tokenised abstracts as follows;\n",
    "1. Bigram creation and collocation using likelihood ratios\n",
    "2. Integration of bigrams into the abstracts, replacing unigram pairs with relevant bigrams where necessary\n",
    "3. Removal of unique tokens, and tokens that are stopwords as determined by either the NLTK Stopwords list, or the Data Wrangling Stopwords list.\n",
    "4. Removal of the top-20 most frequent words\n",
    "5. Transferral of this final collection of terms to the vocabulary file __'vocab.txt'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates bigrams of the patent abstract corpus, filtering out any bigrams where one of the tokens is shorter than 3 letters or\n",
    "# is a stopword in the NLTK stopwords list.\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(flat_token_list)\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in stopwords_NLTK)\n",
    "finder.apply_freq_filter(25) # Ignores all bigrams that occur less than 25 times\n",
    "\n",
    "# Identifies the top 150 bigrams as identified by likelihood ratio. PMI was not chosen as it is sensitive to rare bigrams.\n",
    "# See Boswell, D. (2004) - http://dustwell.com/PastWork/Collocations.pdf\n",
    "bigram_150 = finder.nbest(bigram_measures.likelihood_ratio, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the variable bigrammed_abstracts, which once this block is run will contain a dictionary where they key refers to a\n",
    "# specific patent, while the value is a list containing the tokenised abstract, with the top ******* bigrams replacing relevant\n",
    "# unigrams.\n",
    "bigrammed_abstracts = copy.deepcopy(pat_tokens_raw)\n",
    "\n",
    "# Iterates through each key:value pair in bigrammed_abstracts, checking if each pair of tokens in the abstract is a bigram. If\n",
    "# so, the bigram is appended, and each token is replaced with a nonesense phrase that is removed through a list comprehension\n",
    "# at the end of the process.\n",
    "for key, abstract in bigrammed_abstracts.iteritems():\n",
    "    for token in range(len(bigrammed_abstracts[key])-1):\n",
    "        if (bigrammed_abstracts[key][token], bigrammed_abstracts[key][token+1]) in bigram_150:\n",
    "            append_bigram = (bigrammed_abstracts[key][token]+'_'+bigrammed_abstracts[key][token+1])\n",
    "            bigrammed_abstracts[key].append(append_bigram)\n",
    "            bigrammed_abstracts[key][token] = '*@*'\n",
    "            bigrammed_abstracts[key][token+1] = '*@*'\n",
    "    bigrammed_abstracts[key] = [token for token in bigrammed_abstracts[key] if token != '*@*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns any terms that appear in bigram pairs, as well as the pair, if that term also appears in a list of the\n",
    "# 1*0 most common unigrams after cleaning for stopwords.\n",
    "\n",
    "# Appends each bigram-editted abstract to a list, from which a frequency distribution can be run to establish the top 20 tokens\n",
    "# that will be removed from the vocabulary list. Note that stopwords and single-patent occuring tokens are removed prior to\n",
    "# calculating the frequency distribution.\n",
    "bigrammed_flat_list = []\n",
    "for key, value in bigrammed_abstracts.iteritems():\n",
    "    for token in value:\n",
    "        bigrammed_flat_list.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the variable common_unibigrams, containing the 20 most common tokens (either unigrams or bigrams) in the entirety\n",
    "# of the abstracts. Note that stopwords dropby\n",
    "unibigram_FD = FreqDist([i for i in bigrammed_flat_list if i not in stopwords_NLTK if i not in unique_tokens])\n",
    "common_unibigrams = unibigram_FD.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the 20 most common tokens (unigram and bigrams) from each patent's cleaned abstract - cleaned in that both stopwords,\n",
    "# and words that appear in only one abstract, have been removed.\n",
    "# This results in a dictionary where each key is a patent id and each value a list containing the abstracts for that patent,\n",
    "# with bigrams replacing unigram pairs where relevant, and the patents have been stripped of i. stopwords, ii. the most common\n",
    "# unigrams and bigrams, and iii. the \n",
    "\n",
    "cleanout_list = []\n",
    "cleanout_list += unique_tokens\n",
    "for i in stopwords_NLTK:\n",
    "    cleanout_list.append(i.encode('utf-8'))\n",
    "for i in common_unibigrams:\n",
    "    cleanout_list.append(i[0])\n",
    "                \n",
    "for key, vector in bigrammed_abstracts.iteritems():\n",
    "    bigrammed_abstracts[key] = [i for i in vector if i not in cleanout_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes the finalised abstracts from bigrammed_abstracts and extracts them to a list of lists, which is then flattened and \n",
    "# turned to a set to remove duplicate tokens. This set will be used to create the file 'vocab.txt'.\n",
    "vocab = []\n",
    "for key, value in bigrammed_abstracts.iteritems():\n",
    "    vocab.append(value)    \n",
    "vocab = [i for j in vocab for i in j]\n",
    "vocab = set(vocab)\n",
    "vocab = [i for i in vocab] #Used to strip set formatting from the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a dictionary where each token in the vocab variable becomes a value, with its key being a unique identifier number that\n",
    "# will be referenced in 'vector_count.txt'\n",
    "count = 0\n",
    "vocab_dict = {}\n",
    "for i in vocab:\n",
    "    count += 1\n",
    "    vocab_dict[count] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'vocab.txt' successfully written to file with 5733 lines written.\n"
     ]
    }
   ],
   "source": [
    "# Writes the vocab_dict variable to file.\n",
    "count = 0\n",
    "with open(txt_subdir+\"\\\\vocab.txt\", 'w') as vocab_file:\n",
    "    for i, j in vocab_dict.iteritems():\n",
    "        count += 1\n",
    "        vocab_file.write(str(i)+':'+j+'\\n')\n",
    "        \n",
    "print \"'vocab.txt' successfully written to file with\", count, \"lines written.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate sparse vector count for each patent\n",
    "\n",
    "This section marks the final stage of the assignment, and involves creating a sparse vector count for each patent. This count \n",
    "takes the form $<$patent$>$, $<$vocab_id1$>$:$<$count1$>$, $<$vocab_id2$>$:$<$count2$>$, ... , $<$vocab_id_n$>$:$<$count_n$>$, where each vocab_id refers to \n",
    "an index in the 'vocab.txt' file and the relevant count is for the number of times that vocab_id appears in the bigrammed and cleaned abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Swaps the key:value order of vocab_dict, so that the token id can be more easily accessed.\n",
    "swap_vocab = {j:i for i, j in vocab_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates an empty list variable named count_vector, which will store the final count vector for the assignment. This count\n",
    "# vector is generated by iterating through the bigrammed_abstracts dictionary (the abstracts post-cleaning/post-bigramming), and\n",
    "# creating a list of the vocab identifiers attached to each word in the tokenised abstracts. This list is then run through\n",
    "# Counter to identify the vocab token counts for each patent, and then appended to a list with the patent number as the first\n",
    "# element of the list. This list is then appended to count_vector, which will be read to the 'count_vector.txt' file below.\n",
    "count_vector = []\n",
    "for patent, abstract in bigrammed_abstracts.iteritems():\n",
    "    temp_hold = []\n",
    "    second_hold = []\n",
    "    for element in abstract:\n",
    "        for term, identifier in swap_vocab.iteritems():\n",
    "            if element == term:\n",
    "                temp_hold.append(identifier)\n",
    "    temp_hold = Counter(temp_hold)\n",
    "    second_hold.append(patent+', '+str(temp_hold).strip('Counter({})').replace(': ',':'))\n",
    "    temp_hold = []\n",
    "    count_vector.append(second_hold)\n",
    "    second_hold = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'count_vectors.txt' successfully written to file with 2500 lines written.\n"
     ]
    }
   ],
   "source": [
    "# Writing 'count_vectors.txt' to file, using count_vector variable defined above.\n",
    "count = 0\n",
    "with open(txt_subdir+\"\\\\count_vectors.txt\", 'w') as vector_file:\n",
    "    for i in count_vector:\n",
    "        count += 1\n",
    "        vector_file.write(str(i).strip('\\'[]\\'')+'\\n')\n",
    "        \n",
    "print \"'count_vectors.txt' successfully written to file with\", count, \"lines written.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "This assessment presented a number of challenges to students; while some of them (i.e. the complexity of dealing with a concatenated 'mega XML file') were a great chance for students to develop their skills, there were a number of issues that should be addressed for next semester's assessments.\n",
    "\n",
    "A discussion amongst the teaching staff prior to the release of assignments would ensure that all tutors will be on top of assignment requirements; Mohsen's multiple attempts to double-down on what has been stated by Dickson to be an incorrect interpretation of the assignment description highlights the lack of communication regarding the assignment beforehand by staff. Furthermore, the confusion over whether Python 3.6 would be accepted, and the mixed responses from Rasika and Dickson, further highlight the lack of coordination by staff. While minor in the wider scheme of things, only small changes will need to be made to deal with these issues in the future.\n",
    "\n",
    "In terms of utilising the material covered during the semester's tutorials, I feel that this assignment presents students with a number of ways outside those taught to us to reach the end of the assignment. The count vector above, for example, makes no use of the SKLearn packages. Given that we were offerd ElementTree, lxml, and BeautifulSoup, the lack of resources especially for lxml would have steered a number of students away from these offerings. Integrating the use of modules and packages such as these will force students to develop a better grasp of what is being taught during the semester.\n",
    "\n",
    "Future developments for this class may benefit from also having students place more importance on the collocation method that they chose. While a vast majority of the available resources automatically navigate towards using PMI, alternatives such as likelihood ratio (used here) should not be disregarded especially given the relatively small body of text being examined."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
